{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OtisAlejandro/LLMColabs/blob/main/Otis'_4Bit_Kobold_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to Otis' 4bit Kobold Colab!\n",
        "Mady by OshieteKudasai, Peepy, and Otis\n",
        "Hello everyone and thank you for choosing my colab. If you want to run 13B models but you do not have a good GPU or TPUs aren't working, you can run any compatible 13B model here! This notebook uses Occ4m's 4Bit Kobold Fork which can be foud [here.](https://github.com/0cc4m/KoboldAI)\n",
        "\n",
        "If you'd like more information about Kobold, you can read their Github's readme file here: https://github.com/KoboldAI/KoboldAI-Client/blob/main/readme.md\n",
        "\n",
        "TPUs are currently unavailable, but I'll be making a TPU edition as well for the larger models.\n",
        "\n",
        "If you have any questions or concerns, feel free to DM me @Otis#9664 or ping me in the [PygmalionAI Discord.](discord.gg/pygmalionai)\n",
        "\n",
        "**A special thanks to Peepy and OshieteKudasai for their contributions to this notebook! <3**\n",
        "\n",
        "---\n",
        "## How to load the KoboldAI UI\n",
        "1. Although I designed the colab for desktop users, it should not have a problem on mobile as long as you are using desktop mode. If you're on mobile, tap the play button on the first cell so your Colab doesn't get shut down for inactivity.\n",
        "2. Next, run the second cell to clone the GitHub repository. This will take ~4 minutes to complete, and the cell will stop running automatically.\n",
        "3. Run the 3rd cell to download the model directly from the repo. For now, only Koala is available, however I plan to implement a model selector.\n",
        "4. Run KoboldAI, go to the new_ui. Click interface, and enable Experimental UI. Then, go back to the first page and click 'Load Model'. Select 'load from directory' and select the model you wish to run. After that's done, select 4bit, not 8bit, and wait for it to load! Plug your KoboldAI cloudflare link into your preferred UI, and Chat with 2048 context size, and 12 t/s!\n",
        "4. If you get a message saying no accelerator is available/you ran out of runtime, you are gonna need to wait 24 hours for your cooldown to reset, or try again.\n",
        "5. After everything has loaded, you will get a link to the KoboldAI UI that you can also copy into TavernAI's API and use there. If you get a warning for the localtunnel link, just acknowledge it and proceed to the page. If you get a 1033 error with the Cloudflare link, wait 1 minute and refresh the error page.\n",
        "\n",
        "---\n",
        "\n",
        "Make sure to keep this page open while you are using KoboldAI, and check back regularly to see if you got a Captcha. Failure to complete the captcha's in time can result in termination of your session or a lower priority towards the TPUs.\n",
        "\n",
        "Firefox users need to disable the enhanced tracking protection or use a different browser in order to be able to use Google Colab without errors (This is not something we can do anything about, the cookie blocker breaks the Google Drive integration because it uses different domains).\n",
        "\n",
        "This colab was written completely by hand by Peepy, OshieteKudasai, and Otis. Any copies, or reproductions of this notebook **must** include credit."
      ],
      "metadata": {
        "id": "re1s6eSjDG3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <-- Tap this if you play on Mobile { display-mode: \"form\" }\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive, then start KoboldAI below (Uses only 13MB of data)</b><br/>\n",
        "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "MQvp5Ao1FL01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clone the 4bit KoboldAI Repository and Download LLM (Will take ~7 Minutes to Load)\n",
        "#@markdown Select from a list of available 4bit models.\n",
        "\n",
        "Model = \"Pygmalion 6B 4Bit\" #@param [\"Koala 13B\", \"OpenAssistant 13B (LLaMA)\", \"Pygmalion 6B 4Bit\", \"Llama 13b SUPERCOT 4Bit\", \"Llava 13B 4Bit\", \"GPT4 x Alpaca 13B RolePlayLora 4Bit-v2\", \"GPT4 x Alpaca 13B Lora 4Bit\",  \"GPT4 x Alpaca 13B Native 4Bit\"] {allow-input: true}\n",
        "!apt update\n",
        "!git lfs install\n",
        "!apt install aria2\n",
        "# Clone the repository\n",
        "!git clone https://github.com/0cc4m/KoboldAI -b latestgptq --recurse-submodules\n",
        "#Enter Kobold Dir and run IR with cuda parameter.\n",
        "!cd KoboldAI && ./install_requirements.sh cuda\n",
        "\n",
        "if Model == \"Koala 13B\":\n",
        "  !cd KoboldAI/models && mkdir koala-13B-GPTQ-4bit-128g && cd koala-13B-GPTQ-4bit-128g && aria2c -Z -j4 --summary-interval=5 https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g/resolve/main/.gitattributes https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g/resolve/main/README.md https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g/resolve/main/config.json https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g/resolve/main/generation_config.json https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g/resolve/main/pytorch_model.bin.index.json https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g/resolve/main/special_tokens_map.json https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g/resolve/main/tokenizer.model https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g/resolve/main/tokenizer_config.json https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g/resolve/main/koala-13B-4bit-128g.no-act-order.ooba.pt && mv 9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347 tokenizer.model && mv e80a5b1fdb63334c5d05f0ca3ad756da00937213e7c04ffa554e234f847d290f 4bit-128g.pt \n",
        "elif Model == \"OpenAssistant 13B (LLaMA)\":\n",
        "  !cd KoboldAI/models && git clone https://huggingface.co/gozfarb/oasst-llama13b-4bit-128g && cd oasst-llama-13b-4bit-128g && mv *.safetensors 4bit-128g.safetensors\n",
        "elif Model == \"Pygmalion 6B 4Bit\":\n",
        "  !cd KoboldAI/models && git clone https://huggingface.co/mayaeary/pygmalion-6b-4bit-128g && cd pygmalion-6b-4bit-128g && mv *.safetensors 4bit-128g.safetensors \n",
        "elif Model == \"Llama 13b SUPERCOT 4Bit\":\n",
        "  !cd KoboldAI/models && git clone https://huggingface.co/ausboss/llama-13b-supercot-4bit-128g && cd llama-13b-supercot-4bit-128g && mv *.safetensors 4bit-128g.safetensors\n",
        "elif Model == \"Llava 13B 4Bit\":\n",
        "  !cd KoboldAI/models && git clone https://huggingface.co/wojtab/llava-13b-v0-4bit-128g && cd llava-13b-v0-4bit-129g && mv *.safetensors 4bit-128g.safetensors\n",
        "elif Model == \"GPT4 x Alpaca 13B RolePlayLora 4Bit-v2\":\n",
        "  !cd KoboldAI/models && git clone https://huggingface.co/teknium/GPT4-x-Alpaca13b-RolePlayLora-4bit-v2 && cd GPT4-x-Alpaca13b-RolePlayLora-4bit-v2 && mv *.safetensors 4bit.safetensors && mv *.model tokenizer.model && mv *.bin training_args.bin\n",
        "elif Model == \"GPT4 x Alpaca 13B Lora 4Bit\":\n",
        "  !cd KoboldAI/models && git clone https://huggingface.co/TheBloke/gpt4-alpaca-lora-13B-GPTQ-4bit-128g && cd gpt4-alpaca-lora-13b-GPTQ-4bit-128g && mv gpt4-alpaca-lora-13B-GPTQ-4bit-128g.no-act-order.safetensors 4bit-128g.safetensors\n",
        "elif Model == \"GPT4 x Alpaca 13B Native 4Bit\":\n",
        "  !cd KoboldAI/models && git clone https://huggingface.co/4bit/gpt4-x-alpaca-13b-native-4bit-128g-cuda"
      ],
      "metadata": {
        "id": "vN1jp0sKEpF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Launch Kobold AI (Go to new_ui link and enable Experimental UI in interface settings, then load model in 4bit.)\n",
        "#Enter KAI dir, launch shell script with remote parameter for CloudFlare links\n",
        "Tunnel = \"Localtunnel\" #@param [\"Localtunnel\",\"Cloudflare\"] {allow-input: true}\n",
        "if Tunnel == \"Localtunnel\":\n",
        "  !npm install -g localtunnel\n",
        "  !cd KoboldAI && ./play.sh --localtunnel --quiet\n",
        "else:\n",
        "  !cd KoboldAI && ./play.sh --remote --quiet"
      ],
      "metadata": {
        "id": "e2KO63T5FjlG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}